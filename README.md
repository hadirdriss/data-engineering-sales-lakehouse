# data-engineering-sales-lakehouse

### ğŸ“‹ Project Overview

This project implements a full Data Lakehouse architecture using Databricks, PySpark, and Power BI.
The goal is to build an end-to-end ETL pipeline (Bronze â†’ Silver â†’ Gold) that ingests a CSV file, transforms it, aggregates business metrics, and visualizes results in a Power BI dashboard.

Architecture
![Architecture](data-engineering-sales-lakehouse/diagrams/architecture.jpg)

### ğŸ›  Technologies Used

- Databricks

- PySpark

- SQL

- Delta Lake

- Power BI

- Lakehouse Architecture

- Data Visualization

## ğŸ—ï¸ Project Folder Structure
```
data-engineering-sales-lakehouse/
â”‚
â”œâ”€â”€ databricks/
â”‚     â”œâ”€â”€ bronze_silver_gold_notebook.py
â”‚     â”œâ”€â”€ sql_gold_layer.sql
â”‚
â”œâ”€â”€ powerbi/
â”‚     â”œâ”€â”€ sales_dashboard.pbix
â”‚
â”œâ”€â”€ dataset/
â”‚     â”œâ”€â”€ sample_sales.csv
â”‚
â”œâ”€â”€ diagrams/
â”‚     â”œâ”€â”€ architecture.png
â”‚     â”œâ”€â”€ dashboard_preview.png
â”‚
â””â”€â”€ README.md
```

### ğŸ”„ ETL Pipeline


**1ï¸âƒ£ Bronze Layer**

- Raw ingestion of CSV

- No transformations

- Saved as Delta table

**2ï¸âƒ£ Silver Layer**

- Data cleaning (dropna)

- Date formatting

- Added calculated column total = price * quantity

**3ï¸âƒ£ Gold Layer**

- Aggregation by:

country

product

order_date

- Metrics:

total_sales

total_qty

### Code (Bronze â†’ Silver â†’ Gold)
ğŸ“˜ Bronze â€“ Ingestion
```
df_bronze = (
    spark.read
    .format("csv")
    .option("header", True)
    .option("inferSchema", True)
    .load("/Volumes/demo_catalog/raw/sales_volume/sales_large.csv")
)

df_bronze.write.mode("overwrite").saveAsTable("demo_catalog.raw.bronze_sales")
```
